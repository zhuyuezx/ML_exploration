{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from d2l_helpers import *\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, dropout, **kwargs):\n",
    "        super(AddNorm, self).__init__(**kwargs)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln = nn.LayerNorm(normalized_shape)\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        return self.ln(self.dropout(Y) + X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, key_size, query_size, value_size, num_hiddens, \n",
    "                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,\n",
    "                 dropout, use_bias=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.attention = MultiHeadAttention(key_size, query_size, value_size, num_hiddens, num_heads, dropout, use_bias)\n",
    "        self.addnorm1 = AddNorm(norm_shape, dropout)\n",
    "        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens, num_hiddens)\n",
    "        self.addnorm2 = AddNorm(norm_shape, dropout)\n",
    "    \n",
    "    def forward(self, X, valid_lens):\n",
    "        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))\n",
    "        return self.addnorm2(Y, self.ffn(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEncoder(nn.Module):\n",
    "    \"\"\"BERT编码器\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,\n",
    "                 ffn_num_hiddens, num_heads, num_layers, dropout,\n",
    "                 max_len=1000, key_size=768, query_size=768, value_size=768,\n",
    "                 **kwargs):\n",
    "        super(BERTEncoder, self).__init__(**kwargs)\n",
    "        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)\n",
    "        self.segment_embedding = nn.Embedding(2, num_hiddens)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module(f\"{i}\", EncoderBlock(\n",
    "                key_size, query_size, value_size, num_hiddens, norm_shape,\n",
    "                ffn_num_input, ffn_num_hiddens, num_heads, dropout, True))\n",
    "        # 在BERT中，位置嵌入是可学习的，因此我们创建一个足够长的位置嵌入参数\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, max_len,\n",
    "                                                      num_hiddens))\n",
    "\n",
    "    def forward(self, tokens, segments, valid_lens):\n",
    "        # 在以下代码段中，X的形状保持不变：（批量大小，最大序列长度，num_hiddens）\n",
    "        X = self.token_embedding(tokens) + self.segment_embedding(segments)\n",
    "        X = X + self.pos_embedding.data[:, :X.shape[1], :]\n",
    "        for blk in self.blks:\n",
    "            X = blk(X, valid_lens)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 768])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size, num_hiddens, ffn_num_hiddens, num_heads = 10000, 768, 1024, 4\n",
    "norm_shape, ffn_num_input, num_layers, dropout = [768], 768, 2, 0.2\n",
    "encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape, ffn_num_input,\n",
    "                        ffn_num_hiddens, num_heads, num_layers, dropout)\n",
    "tokens = torch.randint(0, vocab_size, (2, 8))\n",
    "segments = torch.tensor([[0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 1, 1, 1, 1]])\n",
    "encoded_X = encoder(tokens, segments, None)\n",
    "encoded_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskLM(nn.Module):\n",
    "    def __init__(self, vocab_size, num_hiddens, num_inputs=768, **kwargs):\n",
    "        super(MaskLM, self).__init__(**kwargs)\n",
    "        self.mlp = nn.Sequential(nn.Linear(num_inputs, num_hiddens),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.LayerNorm(num_hiddens),\n",
    "                                 nn.Linear(num_hiddens, vocab_size))\n",
    "\n",
    "    def forward(self, X, pred_positions):\n",
    "        num_pred_positions = pred_positions.shape[1]\n",
    "        pred_positions = pred_positions.reshape(-1)\n",
    "        batch_size = X.shape[0]\n",
    "        batch_idx = torch.arange(0, batch_size)\n",
    "        # 假设batch_size=2，num_pred_positions=3\n",
    "        # 那么batch_idx是np.array（[0,0,0,1,1,1]）\n",
    "        batch_idx = torch.repeat_interleave(batch_idx, num_pred_positions)\n",
    "        masked_X = X[batch_idx, pred_positions]\n",
    "        masked_X = masked_X.reshape((batch_size, num_pred_positions, -1))\n",
    "        mlm_Y_hat = self.mlp(masked_X)\n",
    "        return mlm_Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 10000])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm = MaskLM(vocab_size, num_hiddens)\n",
    "mlm_positions = torch.tensor([[1, 5, 2], [6, 1, 5]])\n",
    "mlm_Y_hat = mlm(encoded_X, mlm_positions)\n",
    "mlm_Y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm_Y = torch.tensor([[7, 8, 9], [10, 20, 30]])\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "mlm_l = loss(mlm_Y_hat.reshape((-1, vocab_size)), mlm_Y.reshape(-1))\n",
    "mlm_l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextSentencePred(nn.Module):\n",
    "    def __init__(self, num_inputs, **kwargs):\n",
    "        super(NextSentencePred, self).__init__(**kwargs)\n",
    "        self.output = nn.Linear(num_inputs, 2)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # `X`的形状：(batch_size, `num_hiddens`)\n",
    "        return self.output(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_X = torch.flatten(encoded_X, start_dim=1)\n",
    "# NSP的输入形状:(batchsize，num_hiddens)\n",
    "nsp = NextSentencePred(encoded_X.shape[-1])\n",
    "nsp_Y_hat = nsp(encoded_X)\n",
    "nsp_Y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsp_y = torch.tensor([0, 1])\n",
    "nsp_l = loss(nsp_Y_hat, nsp_y)\n",
    "nsp_l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTModel(nn.Module):\n",
    "    \"\"\"BERT模型\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,\n",
    "                 ffn_num_hiddens, num_heads, num_layers, dropout,\n",
    "                 max_len=1000, key_size=768, query_size=768, value_size=768,\n",
    "                 hid_in_features=768, mlm_in_features=768,\n",
    "                 nsp_in_features=768):\n",
    "        super(BERTModel, self).__init__()\n",
    "        self.encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape,\n",
    "                    ffn_num_input, ffn_num_hiddens, num_heads, num_layers,\n",
    "                    dropout, max_len=max_len, key_size=key_size,\n",
    "                    query_size=query_size, value_size=value_size)\n",
    "        self.hidden = nn.Sequential(nn.Linear(hid_in_features, num_hiddens),\n",
    "                                    nn.Tanh())\n",
    "        self.mlm = MaskLM(vocab_size, num_hiddens, mlm_in_features)\n",
    "        self.nsp = NextSentencePred(nsp_in_features)\n",
    "\n",
    "    def forward(self, tokens, segments, valid_lens=None,\n",
    "                pred_positions=None):\n",
    "        encoded_X = self.encoder(tokens, segments, valid_lens)\n",
    "        if pred_positions is not None:\n",
    "            mlm_Y_hat = self.mlm(encoded_X, pred_positions)\n",
    "        else:\n",
    "            mlm_Y_hat = None\n",
    "        # 用于下一句预测的多层感知机分类器的隐藏层，0是“<cls>”标记的索引\n",
    "        nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, 0, :]))\n",
    "        return encoded_X, mlm_Y_hat, nsp_Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 64]) torch.Size([512, 64]) torch.Size([512]) torch.Size([512, 10]) torch.Size([512, 10]) torch.Size([512, 10]) torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "batch_size, max_len = 512, 64\n",
    "train_iter, vocab = load_data_wiki(batch_size, max_len)\n",
    "\n",
    "for (tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X,\n",
    "     mlm_Y, nsp_y) in train_iter:\n",
    "    print(tokens_X.shape, segments_X.shape, valid_lens_x.shape,\n",
    "          pred_positions_X.shape, mlm_weights_X.shape, mlm_Y.shape,\n",
    "          nsp_y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = BERTModel(len(vocab), num_hiddens=128, norm_shape=[128],\n",
    "                    ffn_num_input=128, ffn_num_hiddens=256, num_heads=2,\n",
    "                    num_layers=2, dropout=0.2, key_size=128, query_size=128,\n",
    "                    value_size=128, hid_in_features=128, mlm_in_features=128,\n",
    "                    nsp_in_features=128)\n",
    "devices = [get_device()]\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_batch_loss_bert(net, loss, vocab_size, tokens_X,\n",
    "                         segments_X, valid_lens_x,\n",
    "                         pred_positions_X, mlm_weights_X,\n",
    "                         mlm_Y, nsp_y):\n",
    "    # 前向传播\n",
    "    _, mlm_Y_hat, nsp_Y_hat = net(tokens_X, segments_X,\n",
    "                                  valid_lens_x.reshape(-1),\n",
    "                                  pred_positions_X)\n",
    "    # 计算遮蔽语言模型损失\n",
    "    mlm_l = loss(mlm_Y_hat.reshape(-1, vocab_size), mlm_Y.reshape(-1)) *\\\n",
    "    mlm_weights_X.reshape(-1, 1)\n",
    "    mlm_l = mlm_l.sum() / (mlm_weights_X.sum() + 1e-8)\n",
    "    # 计算下一句子预测任务的损失\n",
    "    nsp_l = loss(nsp_Y_hat, nsp_y)\n",
    "    l = mlm_l + nsp_l\n",
    "    return mlm_l, nsp_l, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bert(train_iter, net, loss, vocab_size, devices, num_steps):\n",
    "    print(f'training on {devices}')\n",
    "    net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n",
    "    trainer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "    step, timer = 0, Timer()\n",
    "    # 遮蔽语言模型损失的和，下一句预测任务损失的和，句子对的数量，计数\n",
    "    metric = Accumulator(4)\n",
    "    num_steps_reached = False\n",
    "    while step < num_steps and not num_steps_reached:\n",
    "        for tokens_X, segments_X, valid_lens_x, pred_positions_X,\\\n",
    "            mlm_weights_X, mlm_Y, nsp_y in train_iter:\n",
    "            tokens_X = tokens_X.to(devices[0])\n",
    "            segments_X = segments_X.to(devices[0])\n",
    "            valid_lens_x = valid_lens_x.to(devices[0])\n",
    "            pred_positions_X = pred_positions_X.to(devices[0])\n",
    "            mlm_weights_X = mlm_weights_X.to(devices[0])\n",
    "            mlm_Y, nsp_y = mlm_Y.to(devices[0]), nsp_y.to(devices[0])\n",
    "            trainer.zero_grad()\n",
    "            timer.start()\n",
    "            mlm_l, nsp_l, l = _get_batch_loss_bert(\n",
    "                net, loss, vocab_size, tokens_X, segments_X, valid_lens_x,\n",
    "                pred_positions_X, mlm_weights_X, mlm_Y, nsp_y)\n",
    "            l.backward()\n",
    "            trainer.step()\n",
    "            metric.add(mlm_l, nsp_l, tokens_X.shape[0], 1)\n",
    "            timer.stop()\n",
    "            print(f'iter {step}, MLM loss {metric[0] / metric[3]:.3f}, '\n",
    "                  f'NSP loss {metric[1] / metric[3]:.3f}')\n",
    "            step += 1\n",
    "            if step == num_steps:\n",
    "                num_steps_reached = True\n",
    "                break\n",
    "\n",
    "    print(f'MLM loss {metric[0] / metric[3]:.3f}, '\n",
    "          f'NSP loss {metric[1] / metric[3]:.3f}')\n",
    "    print(f'{metric[2] / timer.sum():.1f} sentence pairs/sec on '\n",
    "          f'{str(devices)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on [device(type='mps')]\n",
      "iter 0, MLM loss 9.839, NSP loss 0.709\n",
      "iter 1, MLM loss 9.075, NSP loss 0.832\n",
      "iter 2, MLM loss 8.396, NSP loss 1.126\n",
      "iter 3, MLM loss 7.780, NSP loss 1.030\n",
      "iter 4, MLM loss 7.341, NSP loss 1.005\n",
      "iter 5, MLM loss 7.016, NSP loss 0.954\n",
      "iter 6, MLM loss 6.805, NSP loss 0.920\n",
      "iter 7, MLM loss 6.648, NSP loss 0.904\n",
      "iter 8, MLM loss 6.518, NSP loss 0.882\n",
      "iter 9, MLM loss 6.408, NSP loss 0.866\n",
      "iter 10, MLM loss 6.328, NSP loss 0.856\n",
      "iter 11, MLM loss 6.260, NSP loss 0.846\n",
      "iter 12, MLM loss 6.188, NSP loss 0.836\n",
      "iter 13, MLM loss 6.129, NSP loss 0.830\n",
      "iter 14, MLM loss 6.079, NSP loss 0.823\n",
      "iter 15, MLM loss 6.034, NSP loss 0.815\n",
      "iter 16, MLM loss 5.989, NSP loss 0.810\n",
      "iter 17, MLM loss 5.953, NSP loss 0.805\n",
      "iter 18, MLM loss 5.915, NSP loss 0.799\n",
      "iter 19, MLM loss 5.884, NSP loss 0.794\n",
      "iter 20, MLM loss 5.860, NSP loss 0.789\n",
      "iter 21, MLM loss 5.825, NSP loss 0.786\n",
      "iter 22, MLM loss 5.797, NSP loss 0.782\n",
      "iter 23, MLM loss 5.774, NSP loss 0.778\n",
      "iter 24, MLM loss 5.756, NSP loss 0.775\n",
      "iter 25, MLM loss 5.736, NSP loss 0.773\n",
      "iter 26, MLM loss 5.720, NSP loss 0.770\n",
      "iter 27, MLM loss 5.703, NSP loss 0.767\n",
      "iter 28, MLM loss 5.689, NSP loss 0.765\n",
      "iter 29, MLM loss 5.673, NSP loss 0.763\n",
      "iter 30, MLM loss 5.659, NSP loss 0.761\n",
      "iter 31, MLM loss 5.648, NSP loss 0.760\n",
      "iter 32, MLM loss 5.638, NSP loss 0.758\n",
      "iter 33, MLM loss 5.622, NSP loss 0.756\n",
      "iter 34, MLM loss 5.607, NSP loss 0.755\n",
      "iter 35, MLM loss 5.594, NSP loss 0.753\n",
      "iter 36, MLM loss 5.582, NSP loss 0.752\n",
      "iter 37, MLM loss 5.573, NSP loss 0.750\n",
      "iter 38, MLM loss 5.563, NSP loss 0.749\n",
      "iter 39, MLM loss 5.550, NSP loss 0.748\n",
      "iter 40, MLM loss 5.540, NSP loss 0.746\n",
      "iter 41, MLM loss 5.528, NSP loss 0.745\n",
      "iter 42, MLM loss 5.518, NSP loss 0.744\n",
      "iter 43, MLM loss 5.506, NSP loss 0.743\n",
      "iter 44, MLM loss 5.494, NSP loss 0.742\n",
      "iter 45, MLM loss 5.484, NSP loss 0.741\n",
      "iter 46, MLM loss 5.472, NSP loss 0.740\n",
      "iter 47, MLM loss 5.457, NSP loss 0.739\n",
      "iter 48, MLM loss 5.446, NSP loss 0.739\n",
      "iter 49, MLM loss 5.433, NSP loss 0.738\n",
      "iter 50, MLM loss 5.421, NSP loss 0.737\n",
      "iter 51, MLM loss 5.409, NSP loss 0.736\n",
      "iter 52, MLM loss 5.398, NSP loss 0.735\n",
      "iter 53, MLM loss 5.386, NSP loss 0.735\n",
      "iter 54, MLM loss 5.379, NSP loss 0.734\n",
      "iter 55, MLM loss 5.369, NSP loss 0.733\n",
      "iter 56, MLM loss 5.358, NSP loss 0.733\n",
      "iter 57, MLM loss 5.348, NSP loss 0.732\n",
      "iter 58, MLM loss 5.340, NSP loss 0.732\n",
      "iter 59, MLM loss 5.333, NSP loss 0.731\n",
      "iter 60, MLM loss 5.323, NSP loss 0.731\n",
      "iter 61, MLM loss 5.315, NSP loss 0.730\n",
      "iter 62, MLM loss 5.307, NSP loss 0.730\n",
      "iter 63, MLM loss 5.299, NSP loss 0.729\n",
      "iter 64, MLM loss 5.292, NSP loss 0.729\n",
      "iter 65, MLM loss 5.283, NSP loss 0.728\n",
      "iter 66, MLM loss 5.280, NSP loss 0.728\n",
      "iter 67, MLM loss 5.279, NSP loss 0.727\n",
      "iter 68, MLM loss 5.276, NSP loss 0.727\n",
      "iter 69, MLM loss 5.270, NSP loss 0.726\n",
      "iter 70, MLM loss 5.265, NSP loss 0.726\n",
      "iter 71, MLM loss 5.260, NSP loss 0.725\n",
      "iter 72, MLM loss 5.252, NSP loss 0.725\n",
      "iter 73, MLM loss 5.247, NSP loss 0.724\n",
      "iter 74, MLM loss 5.241, NSP loss 0.724\n",
      "iter 75, MLM loss 5.237, NSP loss 0.724\n",
      "iter 76, MLM loss 5.231, NSP loss 0.723\n",
      "iter 77, MLM loss 5.227, NSP loss 0.723\n",
      "iter 78, MLM loss 5.222, NSP loss 0.723\n",
      "iter 79, MLM loss 5.216, NSP loss 0.722\n",
      "iter 80, MLM loss 5.212, NSP loss 0.722\n",
      "iter 81, MLM loss 5.207, NSP loss 0.722\n",
      "iter 82, MLM loss 5.200, NSP loss 0.721\n",
      "iter 83, MLM loss 5.196, NSP loss 0.721\n",
      "iter 84, MLM loss 5.191, NSP loss 0.721\n",
      "iter 85, MLM loss 5.186, NSP loss 0.720\n",
      "iter 86, MLM loss 5.183, NSP loss 0.720\n",
      "iter 87, MLM loss 5.178, NSP loss 0.720\n",
      "iter 88, MLM loss 5.173, NSP loss 0.719\n",
      "iter 89, MLM loss 5.168, NSP loss 0.719\n",
      "iter 90, MLM loss 5.165, NSP loss 0.719\n",
      "iter 91, MLM loss 5.164, NSP loss 0.719\n",
      "iter 92, MLM loss 5.166, NSP loss 0.718\n",
      "iter 93, MLM loss 5.168, NSP loss 0.718\n",
      "iter 94, MLM loss 5.170, NSP loss 0.718\n",
      "iter 95, MLM loss 5.171, NSP loss 0.717\n",
      "iter 96, MLM loss 5.171, NSP loss 0.717\n",
      "iter 97, MLM loss 5.171, NSP loss 0.717\n",
      "iter 98, MLM loss 5.171, NSP loss 0.717\n",
      "iter 99, MLM loss 5.171, NSP loss 0.717\n",
      "iter 100, MLM loss 5.172, NSP loss 0.717\n",
      "iter 101, MLM loss 5.172, NSP loss 0.717\n",
      "iter 102, MLM loss 5.173, NSP loss 0.716\n",
      "iter 103, MLM loss 5.172, NSP loss 0.716\n",
      "iter 104, MLM loss 5.171, NSP loss 0.716\n",
      "iter 105, MLM loss 5.171, NSP loss 0.716\n",
      "iter 106, MLM loss 5.170, NSP loss 0.716\n",
      "iter 107, MLM loss 5.170, NSP loss 0.715\n",
      "iter 108, MLM loss 5.169, NSP loss 0.715\n",
      "iter 109, MLM loss 5.168, NSP loss 0.715\n",
      "iter 110, MLM loss 5.167, NSP loss 0.715\n",
      "iter 111, MLM loss 5.166, NSP loss 0.715\n",
      "iter 112, MLM loss 5.164, NSP loss 0.715\n",
      "iter 113, MLM loss 5.161, NSP loss 0.715\n",
      "iter 114, MLM loss 5.161, NSP loss 0.715\n",
      "iter 115, MLM loss 5.158, NSP loss 0.715\n",
      "iter 116, MLM loss 5.159, NSP loss 0.715\n",
      "iter 117, MLM loss 5.157, NSP loss 0.715\n",
      "iter 118, MLM loss 5.155, NSP loss 0.714\n",
      "iter 119, MLM loss 5.154, NSP loss 0.714\n",
      "iter 120, MLM loss 5.153, NSP loss 0.714\n",
      "iter 121, MLM loss 5.152, NSP loss 0.714\n",
      "iter 122, MLM loss 5.151, NSP loss 0.714\n",
      "iter 123, MLM loss 5.150, NSP loss 0.714\n",
      "iter 124, MLM loss 5.149, NSP loss 0.714\n",
      "iter 125, MLM loss 5.148, NSP loss 0.714\n",
      "iter 126, MLM loss 5.146, NSP loss 0.714\n",
      "iter 127, MLM loss 5.145, NSP loss 0.714\n",
      "iter 128, MLM loss 5.144, NSP loss 0.714\n",
      "iter 129, MLM loss 5.143, NSP loss 0.713\n",
      "iter 130, MLM loss 5.143, NSP loss 0.713\n",
      "iter 131, MLM loss 5.142, NSP loss 0.713\n",
      "iter 132, MLM loss 5.142, NSP loss 0.713\n",
      "iter 133, MLM loss 5.140, NSP loss 0.713\n",
      "iter 134, MLM loss 5.140, NSP loss 0.713\n",
      "iter 135, MLM loss 5.139, NSP loss 0.713\n",
      "iter 136, MLM loss 5.138, NSP loss 0.713\n",
      "iter 137, MLM loss 5.137, NSP loss 0.713\n",
      "iter 138, MLM loss 5.136, NSP loss 0.713\n",
      "iter 139, MLM loss 5.135, NSP loss 0.712\n",
      "iter 140, MLM loss 5.134, NSP loss 0.712\n",
      "iter 141, MLM loss 5.134, NSP loss 0.712\n",
      "iter 142, MLM loss 5.132, NSP loss 0.712\n",
      "iter 143, MLM loss 5.132, NSP loss 0.712\n",
      "iter 144, MLM loss 5.131, NSP loss 0.712\n",
      "iter 145, MLM loss 5.130, NSP loss 0.712\n",
      "iter 146, MLM loss 5.130, NSP loss 0.712\n",
      "iter 147, MLM loss 5.129, NSP loss 0.711\n",
      "iter 148, MLM loss 5.128, NSP loss 0.711\n",
      "iter 149, MLM loss 5.128, NSP loss 0.711\n",
      "iter 150, MLM loss 5.128, NSP loss 0.711\n",
      "iter 151, MLM loss 5.126, NSP loss 0.711\n",
      "iter 152, MLM loss 5.126, NSP loss 0.711\n",
      "iter 153, MLM loss 5.125, NSP loss 0.711\n",
      "iter 154, MLM loss 5.124, NSP loss 0.711\n",
      "iter 155, MLM loss 5.123, NSP loss 0.711\n",
      "iter 156, MLM loss 5.122, NSP loss 0.711\n",
      "iter 157, MLM loss 5.121, NSP loss 0.710\n",
      "iter 158, MLM loss 5.120, NSP loss 0.710\n",
      "iter 159, MLM loss 5.119, NSP loss 0.710\n",
      "iter 160, MLM loss 5.118, NSP loss 0.710\n",
      "iter 161, MLM loss 5.118, NSP loss 0.710\n",
      "iter 162, MLM loss 5.117, NSP loss 0.710\n",
      "iter 163, MLM loss 5.115, NSP loss 0.710\n",
      "iter 164, MLM loss 5.114, NSP loss 0.710\n",
      "iter 165, MLM loss 5.113, NSP loss 0.710\n",
      "iter 166, MLM loss 5.112, NSP loss 0.710\n",
      "iter 167, MLM loss 5.112, NSP loss 0.710\n",
      "iter 168, MLM loss 5.112, NSP loss 0.710\n",
      "iter 169, MLM loss 5.111, NSP loss 0.709\n",
      "iter 170, MLM loss 5.110, NSP loss 0.709\n",
      "iter 171, MLM loss 5.109, NSP loss 0.709\n",
      "iter 172, MLM loss 5.108, NSP loss 0.709\n",
      "iter 173, MLM loss 5.107, NSP loss 0.709\n",
      "iter 174, MLM loss 5.106, NSP loss 0.709\n",
      "iter 175, MLM loss 5.106, NSP loss 0.709\n",
      "iter 176, MLM loss 5.106, NSP loss 0.709\n",
      "iter 177, MLM loss 5.105, NSP loss 0.709\n",
      "iter 178, MLM loss 5.104, NSP loss 0.709\n",
      "iter 179, MLM loss 5.103, NSP loss 0.709\n",
      "iter 180, MLM loss 5.102, NSP loss 0.709\n",
      "iter 181, MLM loss 5.102, NSP loss 0.709\n",
      "iter 182, MLM loss 5.102, NSP loss 0.709\n",
      "iter 183, MLM loss 5.101, NSP loss 0.708\n",
      "iter 184, MLM loss 5.101, NSP loss 0.708\n",
      "iter 185, MLM loss 5.101, NSP loss 0.708\n",
      "iter 186, MLM loss 5.100, NSP loss 0.708\n",
      "iter 187, MLM loss 5.100, NSP loss 0.708\n",
      "iter 188, MLM loss 5.099, NSP loss 0.708\n",
      "iter 189, MLM loss 5.098, NSP loss 0.708\n",
      "iter 190, MLM loss 5.097, NSP loss 0.708\n",
      "iter 191, MLM loss 5.097, NSP loss 0.708\n",
      "iter 192, MLM loss 5.097, NSP loss 0.708\n",
      "iter 193, MLM loss 5.097, NSP loss 0.708\n",
      "iter 194, MLM loss 5.097, NSP loss 0.708\n",
      "iter 195, MLM loss 5.096, NSP loss 0.708\n",
      "iter 196, MLM loss 5.096, NSP loss 0.708\n",
      "iter 197, MLM loss 5.096, NSP loss 0.708\n",
      "iter 198, MLM loss 5.095, NSP loss 0.708\n",
      "iter 199, MLM loss 5.095, NSP loss 0.708\n",
      "iter 200, MLM loss 5.094, NSP loss 0.708\n",
      "iter 201, MLM loss 5.094, NSP loss 0.708\n",
      "iter 202, MLM loss 5.094, NSP loss 0.707\n",
      "iter 203, MLM loss 5.093, NSP loss 0.707\n",
      "iter 204, MLM loss 5.093, NSP loss 0.707\n",
      "iter 205, MLM loss 5.092, NSP loss 0.707\n",
      "iter 206, MLM loss 5.092, NSP loss 0.707\n",
      "iter 207, MLM loss 5.092, NSP loss 0.707\n",
      "iter 208, MLM loss 5.091, NSP loss 0.707\n",
      "iter 209, MLM loss 5.090, NSP loss 0.707\n",
      "iter 210, MLM loss 5.090, NSP loss 0.707\n",
      "iter 211, MLM loss 5.089, NSP loss 0.707\n",
      "iter 212, MLM loss 5.089, NSP loss 0.707\n",
      "iter 213, MLM loss 5.089, NSP loss 0.707\n",
      "iter 214, MLM loss 5.088, NSP loss 0.707\n",
      "iter 215, MLM loss 5.088, NSP loss 0.707\n",
      "iter 216, MLM loss 5.087, NSP loss 0.707\n",
      "iter 217, MLM loss 5.087, NSP loss 0.707\n",
      "iter 218, MLM loss 5.086, NSP loss 0.707\n",
      "iter 219, MLM loss 5.086, NSP loss 0.707\n",
      "iter 220, MLM loss 5.085, NSP loss 0.707\n",
      "iter 221, MLM loss 5.085, NSP loss 0.707\n",
      "iter 222, MLM loss 5.085, NSP loss 0.707\n",
      "iter 223, MLM loss 5.084, NSP loss 0.707\n",
      "iter 224, MLM loss 5.084, NSP loss 0.707\n",
      "iter 225, MLM loss 5.082, NSP loss 0.707\n",
      "iter 226, MLM loss 5.082, NSP loss 0.707\n",
      "iter 227, MLM loss 5.082, NSP loss 0.707\n",
      "iter 228, MLM loss 5.082, NSP loss 0.707\n",
      "iter 229, MLM loss 5.082, NSP loss 0.707\n",
      "iter 230, MLM loss 5.081, NSP loss 0.707\n",
      "iter 231, MLM loss 5.081, NSP loss 0.707\n",
      "iter 232, MLM loss 5.080, NSP loss 0.707\n",
      "iter 233, MLM loss 5.080, NSP loss 0.707\n",
      "iter 234, MLM loss 5.080, NSP loss 0.707\n",
      "iter 235, MLM loss 5.079, NSP loss 0.707\n",
      "iter 236, MLM loss 5.079, NSP loss 0.707\n",
      "iter 237, MLM loss 5.079, NSP loss 0.707\n",
      "iter 238, MLM loss 5.079, NSP loss 0.707\n",
      "iter 239, MLM loss 5.078, NSP loss 0.707\n",
      "iter 240, MLM loss 5.078, NSP loss 0.707\n",
      "iter 241, MLM loss 5.078, NSP loss 0.707\n",
      "iter 242, MLM loss 5.078, NSP loss 0.706\n",
      "iter 243, MLM loss 5.078, NSP loss 0.706\n",
      "iter 244, MLM loss 5.077, NSP loss 0.706\n",
      "iter 245, MLM loss 5.077, NSP loss 0.706\n",
      "iter 246, MLM loss 5.076, NSP loss 0.706\n",
      "iter 247, MLM loss 5.076, NSP loss 0.706\n",
      "iter 248, MLM loss 5.075, NSP loss 0.706\n",
      "iter 249, MLM loss 5.075, NSP loss 0.706\n",
      "iter 250, MLM loss 5.074, NSP loss 0.706\n",
      "iter 251, MLM loss 5.074, NSP loss 0.706\n",
      "iter 252, MLM loss 5.074, NSP loss 0.706\n",
      "iter 253, MLM loss 5.074, NSP loss 0.706\n",
      "iter 254, MLM loss 5.073, NSP loss 0.706\n",
      "iter 255, MLM loss 5.073, NSP loss 0.706\n",
      "iter 256, MLM loss 5.073, NSP loss 0.706\n",
      "iter 257, MLM loss 5.072, NSP loss 0.706\n",
      "iter 258, MLM loss 5.071, NSP loss 0.706\n",
      "iter 259, MLM loss 5.071, NSP loss 0.706\n",
      "iter 260, MLM loss 5.070, NSP loss 0.706\n",
      "iter 261, MLM loss 5.070, NSP loss 0.706\n",
      "iter 262, MLM loss 5.070, NSP loss 0.706\n",
      "iter 263, MLM loss 5.069, NSP loss 0.706\n",
      "iter 264, MLM loss 5.069, NSP loss 0.706\n",
      "iter 265, MLM loss 5.069, NSP loss 0.706\n",
      "iter 266, MLM loss 5.068, NSP loss 0.706\n",
      "iter 267, MLM loss 5.068, NSP loss 0.706\n",
      "iter 268, MLM loss 5.068, NSP loss 0.706\n",
      "iter 269, MLM loss 5.067, NSP loss 0.706\n",
      "iter 270, MLM loss 5.067, NSP loss 0.706\n",
      "iter 271, MLM loss 5.067, NSP loss 0.706\n",
      "iter 272, MLM loss 5.067, NSP loss 0.705\n",
      "iter 273, MLM loss 5.066, NSP loss 0.705\n",
      "iter 274, MLM loss 5.066, NSP loss 0.705\n",
      "iter 275, MLM loss 5.066, NSP loss 0.705\n",
      "iter 276, MLM loss 5.066, NSP loss 0.705\n",
      "iter 277, MLM loss 5.066, NSP loss 0.705\n",
      "iter 278, MLM loss 5.065, NSP loss 0.705\n",
      "iter 279, MLM loss 5.064, NSP loss 0.705\n",
      "iter 280, MLM loss 5.064, NSP loss 0.705\n",
      "iter 281, MLM loss 5.064, NSP loss 0.705\n",
      "iter 282, MLM loss 5.063, NSP loss 0.705\n",
      "iter 283, MLM loss 5.063, NSP loss 0.705\n",
      "iter 284, MLM loss 5.063, NSP loss 0.705\n",
      "iter 285, MLM loss 5.062, NSP loss 0.705\n",
      "iter 286, MLM loss 5.062, NSP loss 0.705\n",
      "iter 287, MLM loss 5.061, NSP loss 0.705\n",
      "iter 288, MLM loss 5.061, NSP loss 0.705\n",
      "iter 289, MLM loss 5.060, NSP loss 0.705\n",
      "iter 290, MLM loss 5.060, NSP loss 0.705\n",
      "iter 291, MLM loss 5.059, NSP loss 0.705\n",
      "iter 292, MLM loss 5.059, NSP loss 0.705\n",
      "iter 293, MLM loss 5.059, NSP loss 0.705\n",
      "iter 294, MLM loss 5.059, NSP loss 0.705\n",
      "iter 295, MLM loss 5.058, NSP loss 0.705\n",
      "iter 296, MLM loss 5.058, NSP loss 0.705\n",
      "iter 297, MLM loss 5.058, NSP loss 0.705\n",
      "iter 298, MLM loss 5.057, NSP loss 0.705\n",
      "iter 299, MLM loss 5.057, NSP loss 0.705\n",
      "MLM loss 5.057, NSP loss 0.705\n",
      "5192.7 sentence pairs/sec on [device(type='mps')]\n"
     ]
    }
   ],
   "source": [
    "train_bert(train_iter, net, loss, len(vocab), devices, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_encoding(net, tokens_a, tokens_b=None):\n",
    "    tokens, segments = get_tokens_and_segments(tokens_a, tokens_b)\n",
    "    token_ids = torch.tensor(vocab[tokens], device=devices[0]).unsqueeze(0)\n",
    "    segments = torch.tensor(segments, device=devices[0]).unsqueeze(0)\n",
    "    valid_len = torch.tensor(len(tokens), device=devices[0]).unsqueeze(0)\n",
    "    encoded_X, _, _ = net(token_ids, segments, valid_len)\n",
    "    return encoded_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 6, 128]),\n",
       " torch.Size([1, 128]),\n",
       " tensor([-0.8455, -1.0833,  0.7007], device='mps:0', grad_fn=<SliceBackward0>))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_a = ['a', 'crane', 'is', 'flying']\n",
    "encoded_text = get_bert_encoding(net, tokens_a)\n",
    "# 词元：'<cls>','a','crane','is','flying','<sep>'\n",
    "encoded_text_cls = encoded_text[:, 0, :]\n",
    "encoded_text_crane = encoded_text[:, 2, :]\n",
    "encoded_text.shape, encoded_text_cls.shape, encoded_text_crane[0][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 10, 128]),\n",
       " torch.Size([1, 128]),\n",
       " tensor([-0.0166, -2.2550,  0.1014], device='mps:0', grad_fn=<SliceBackward0>))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_a, tokens_b = ['a', 'crane', 'driver', 'came'], ['he', 'just', 'left']\n",
    "encoded_pair = get_bert_encoding(net, tokens_a, tokens_b)\n",
    "# 词元：'<cls>','a','crane','driver','came','<sep>','he','just',\n",
    "# 'left','<sep>'\n",
    "encoded_pair_cls = encoded_pair[:, 0, :]\n",
    "encoded_pair_crane = encoded_pair[:, 2, :]\n",
    "encoded_pair.shape, encoded_pair_cls.shape, encoded_pair_crane[0][:3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
