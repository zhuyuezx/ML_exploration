{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from d2l_helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = load_data_time_machine(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 28])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(10).reshape((2, 5))\n",
    "F.one_hot(X.T, 28).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(vocab_size, num_hiddens, device):\n",
    "    num_inputs = num_outputs = vocab_size\n",
    "\n",
    "    def normal(shape):\n",
    "        return torch.randn(size=shape, device=device) * 0.01\n",
    "\n",
    "    # 隐藏层参数\n",
    "    W_xh = normal((num_inputs, num_hiddens))\n",
    "    W_hh = normal((num_hiddens, num_hiddens))\n",
    "    b_h = torch.zeros(num_hiddens, device=device)\n",
    "    # 输出层参数\n",
    "    W_hq = normal((num_hiddens, num_outputs))\n",
    "    b_q = torch.zeros(num_outputs, device=device)\n",
    "    # 附加梯度\n",
    "    params = [W_xh, W_hh, b_h, W_hq, b_q]\n",
    "    for param in params:\n",
    "        param.requires_grad_(True)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_rnn_state(batch_size, num_hiddens, device):\n",
    "    return (torch.zeros((batch_size, num_hiddens), device=device), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn(inputs, state, params):\n",
    "    # inputs的形状：(时间步数量，批量大小，词表大小)\n",
    "    W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    H, = state\n",
    "    outputs = []\n",
    "    # X的形状：(批量大小，词表大小)\n",
    "    for X in inputs:\n",
    "        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)\n",
    "        Y = torch.mm(H, W_hq) + b_q\n",
    "        outputs.append(Y)\n",
    "    return torch.cat(outputs, dim=0), (H,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModelScratch: #@save\n",
    "    \"\"\"从零开始实现的循环神经网络模型\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, device,\n",
    "                 get_params, init_state, forward_fn):\n",
    "        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens\n",
    "        self.params = get_params(vocab_size, num_hiddens, device)\n",
    "        self.init_state, self.forward_fn = init_state, forward_fn\n",
    "\n",
    "    def __call__(self, X, state):\n",
    "        X = F.one_hot(X.T, self.vocab_size).type(torch.float32)\n",
    "        return self.forward_fn(X, state, self.params)\n",
    "\n",
    "    def begin_state(self, batch_size, device):\n",
    "        return self.init_state(batch_size, self.num_hiddens, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 28]), 1, torch.Size([2, 512]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_hiddens = 512\n",
    "net = RNNModelScratch(len(vocab), num_hiddens, get_device(), get_params,\n",
    "                      init_rnn_state, rnn)\n",
    "state = net.begin_state(X.shape[0], get_device())\n",
    "Y, new_state = net(X.to(get_device()), state)\n",
    "Y.shape, len(new_state), new_state[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'time travellerrrrrrrrrrr'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_rnn('time traveller', 10, net, vocab, get_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time traveller the the the the the the the the the the the the t\n",
      "epoch 10, perplexity 13.6\n",
      "time travellere the the the the the the the the the the the the \n",
      "epoch 20, perplexity 10.6\n",
      "time travellere the the the the the the the the the the the the \n",
      "epoch 30, perplexity 9.6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m num_epochs, lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m, \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain_rnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/U_of_T/Winter_5th_year/ML_Exploration/d2l_helpers.py:397\u001b[0m, in \u001b[0;36mtrain_rnn\u001b[0;34m(net, train_iter, vocab, lr, num_epochs, device, use_random_iter)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    396\u001b[0m     updater \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m batch_size: sgd(net\u001b[38;5;241m.\u001b[39mparams, lr, batch_size)\n\u001b[0;32m--> 397\u001b[0m predict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m prefix: predict_rnn(prefix, \u001b[38;5;241m50\u001b[39m, net, vocab, device)\n\u001b[1;32m    398\u001b[0m \u001b[38;5;66;03m# 训练和预测\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n",
      "File \u001b[0;32m~/Documents/U_of_T/Winter_5th_year/ML_Exploration/d2l_helpers.py:379\u001b[0m, in \u001b[0;36mtrain_epoch_rnn\u001b[0;34m(net, train_iter, loss, updater, device, use_random_iter)\u001b[0m\n\u001b[1;32m    377\u001b[0m     l\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    378\u001b[0m     grad_clipping(net, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 379\u001b[0m     updater\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m     l\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs, lr = 500, 1\n",
    "train_rnn(net, train_iter, vocab, lr, num_epochs, get_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time travellerit s against reason said the medical manour ancest\n",
      "epoch 10, perplexity 2.0\n",
      "time travellerit s against reason said filbycan ascuble he have \n",
      "epoch 20, perplexity 1.7\n",
      "time travellerit s against reason said filby but you willnever c\n",
      "epoch 30, perplexity 1.8\n",
      "time traveller proc ensong bre weing wisereell simely thing he t\n",
      "epoch 40, perplexity 1.8\n",
      "time traveller but now you begin to seethe object of my investig\n",
      "epoch 50, perplexity 1.6\n",
      "time travellerit s alain th saldithis ction at reger mace ane or\n",
      "epoch 60, perplexity 1.8\n",
      "time travellerit s againstarthere and for the frese suin ceqvedi\n",
      "epoch 70, perplexity 1.6\n",
      "time travellerit s against reason said filbychat whis ho d wist \n",
      "epoch 80, perplexity 1.6\n",
      "time traveller proceeded anyreal body must have extension in fou\n",
      "epoch 90, perplexity 1.6\n",
      "time traveller proceeded anyreal body must havelength breadth th\n",
      "epoch 100, perplexity 1.5\n",
      "time travellerit s against reason said filbywon a kfowr his regl\n",
      "epoch 110, perplexity 1.7\n",
      "time traveller but now you begin to seethe object of my investig\n",
      "epoch 120, perplexity 1.7\n",
      "time travellerit s against reason said filbywhat whepense has oo\n",
      "epoch 130, perplexity 1.4\n",
      "time travellerit s against reason said filbycan a cube that does\n",
      "epoch 140, perplexity 1.5\n",
      "time travellerit s against reason said filbycan a cube that does\n",
      "epoch 150, perplexity 1.5\n",
      "time travellerit s against reason said filbycan a cube that does\n",
      "epoch 160, perplexity 1.5\n",
      "time traveller held in his hand was a glitteringmetallic framewo\n",
      "epoch 170, perplexity 1.4\n",
      "time travellerit s against reason said filby of course a solid b\n",
      "epoch 180, perplexity 1.6\n",
      "time travellerit s against reason said filbywhat wave th meait f\n",
      "epoch 190, perplexity 1.5\n",
      "time traveller held in his hand was a glitteringmetallic framewo\n",
      "epoch 200, perplexity 1.4\n",
      "time travellerit s against reason said filbywhat of hththimen an\n",
      "epoch 210, perplexity 1.4\n",
      "time traveller proceeded anyreal body must have extension in fou\n",
      "epoch 220, perplexity 1.3\n",
      "time traveller held in his hand was a glitteringmetallic framewo\n",
      "epoch 230, perplexity 1.4\n",
      "time travellerit s against reason said filbywhof cours metainel \n",
      "epoch 240, perplexity 1.5\n",
      "time travellerit s against reason said filbywhat whee antwon  tr\n",
      "epoch 250, perplexity 1.3\n",
      "time travellerit s against reason said filbywhit said and his fo\n",
      "epoch 260, perplexity 1.5\n",
      "time traveller but now you begin to seethe object of my investig\n",
      "epoch 270, perplexity 1.4\n",
      "time traveller held in his hand was a glitteringmetallic framewo\n",
      "epoch 280, perplexity 1.4\n",
      "time traveller held in his hand was a glitteringmetallic framewo\n",
      "epoch 290, perplexity 1.5\n",
      "time traveller proceeded anyreal body must have extension in fou\n",
      "epoch 300, perplexity 1.5\n",
      "time traveller held in his hand was a glitteringmetallic framewo\n",
      "epoch 310, perplexity 1.4\n",
      "time travellerit s against reason said filbywhat whed a fourth d\n",
      "epoch 320, perplexity 1.4\n",
      "time travellerit s against reason said filbycan a cube that does\n",
      "epoch 330, perplexity 1.3\n",
      "time traveller held in his hand was a glitteringmetallic framewo\n",
      "epoch 340, perplexity 1.4\n",
      "time travellerit s against reason said filbywhat whes not hase o\n",
      "epoch 350, perplexity 1.3\n",
      "time travellerit s against reason said filbywhat whee a said the\n",
      "epoch 360, perplexity 1.4\n",
      "time traveller proceeded anyreal body must have extension in fou\n",
      "epoch 370, perplexity 1.3\n",
      "time traveller held in his hand was a glitteringmetallic framewo\n",
      "epoch 380, perplexity 1.3\n",
      "time traveller held in his hand was a glitteringmetallic framewo\n",
      "epoch 390, perplexity 1.3\n",
      "time travellerit s against reason said filbywhat waid the gimmpt\n",
      "epoch 400, perplexity 1.3\n",
      "time traveller held in his hand was a glitteringmetallic framewo\n",
      "epoch 410, perplexity 1.3\n",
      "time traveller held in his hand was a glitteringmetallic framewo\n",
      "epoch 420, perplexity 1.4\n",
      "time traveller held in his hand was a glitteringmetallic framewo\n",
      "epoch 430, perplexity 1.3\n",
      "time traveller held in his hand was a glitteringmetallic framewo\n",
      "epoch 440, perplexity 1.4\n",
      "time travellerit s against reason said filbywhat whed ho daws th\n",
      "epoch 450, perplexity 1.4\n",
      "time traveller but now you begin to seethe object of my investig\n",
      "epoch 460, perplexity 1.3\n",
      "time traveller held in his hand was a glitteringmetallic framewo\n",
      "epoch 470, perplexity 1.3\n",
      "time traveller held in his hand was a glitteringmetallic framewo\n",
      "epoch 480, perplexity 1.3\n",
      "time travellerit s against reason said filbywhat weed ho  a mat \n",
      "epoch 490, perplexity 1.3\n",
      "time traveller held in his hand was a glitteringmetallic framewo\n",
      "epoch 500, perplexity 1.3\n",
      "ppl: 1.3, 68567.5 token/s: mps\n",
      "time traveller held in his hand was a glitteringmetallic framewo\n",
      "travellerit s against reason said filbywhat whid is plothe \n"
     ]
    }
   ],
   "source": [
    "train_rnn(net, train_iter, vocab, lr, num_epochs, get_device(), use_random_iter=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
